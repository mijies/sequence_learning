{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.initializers import glorot_uniform as keras_glorot_uniform\n",
    "from keras.initializers import orthogonal as keras_orthogonal\n",
    "from keras.initializers import uniform as keras_uniform\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Masking\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('data/words.pickle', 'rb') as f:\n",
    "    words = pickle.load(f)\n",
    "    \n",
    "enc_input = np.load('data/enc_input.npy')\n",
    "dec_input = np.load('data/dec_input.npy')\n",
    "lbl_input = np.load('data/lbl_input.npy')\n",
    "maxlen_e, maxlen_d = np.load('data/maxlen.npy')\n",
    "\n",
    "\n",
    "# Split data for training and test\n",
    "sep_idx = int(enc_input.shape[0] * 0.95)\n",
    "enc_train, enc_test = np.vsplit(enc_input, [sep_idx])\n",
    "dec_train, dec_test = np.vsplit(dec_input, [sep_idx])\n",
    "lbl_train, lbl_test = np.vsplit(lbl_input, [sep_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dialog:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        maxlen_e,\n",
    "        maxlen_d,\n",
    "        num_input,\n",
    "        num_output,\n",
    "        dim_vec,\n",
    "        dim_hidden_vec\n",
    "    ):\n",
    "        self.maxlen_e = maxlen_e\n",
    "        self.maxlen_d = maxlen_d\n",
    "        self.num_input = num_input\n",
    "        self.num_output = num_output\n",
    "        self.dim_vec = dim_vec\n",
    "        self.dim_hidden_vec = dim_hidden_vec\n",
    "        \n",
    "    def create_model(self):        # Used in train() and prediction\n",
    "        \n",
    "        print('\\n...Start creating models.\\n')\n",
    "        \n",
    "        #\n",
    "        # Encoder model creation\n",
    "        #\n",
    "        \n",
    "        enc_input = Input(shape=(self.maxlen_e,), dtype='int32', name='encoder_input')\n",
    "        tf_tensor = Embedding(\n",
    "                              input_dim=self.num_input, \n",
    "                              output_dim=self.num_output, \n",
    "                              mask_zero=True, # ID 0 in Input data is considered as padding.\n",
    "                              embeddings_initializer=keras_uniform(seed=12345)\n",
    "                             )(enc_input)\n",
    "        \n",
    "        # axis -1 counts dim from the highest rank\n",
    "        tf_tensor = BatchNormalization(axis=-1)(tf_tensor)\n",
    "        tf_tensor = Masking(mask_value=0.0)(tf_tensor)\n",
    "        \n",
    "        enc_output, enc_hidden_state, enc_cell_state = LSTM(\n",
    "                                                            units=self.dim_hidden_vec,\n",
    "                                                            kernel_initializer=keras_glorot_uniform(seed=12345),\n",
    "                                                            # Coefficient to the orthogonal matrix is 1.0\n",
    "                                                            recurrent_initializer=keras_orthogonal(gain=1.0, seed=12345),\n",
    "                                                            dropout=0.5,\n",
    "                                                            recurrent_dropout=0.5,\n",
    "                                                            return_state=True,\n",
    "                                                           )(tf_tensor)\n",
    "        \n",
    "        enc_model  = Model(inputs=enc_input, outputs=[enc_output, enc_hidden_state, enc_cell_state])\n",
    "\n",
    "        print('Encoder model created.\\n')\n",
    "        \n",
    "        #\n",
    "        # Decoder training model creation\n",
    "        #\n",
    "        \n",
    "        dec_input = Input(shape=(self.maxlen_d,), dtype='int32', name='decoder_input')\n",
    "        tf_tensor = Embedding(\n",
    "                              input_dim=self.num_input,\n",
    "                              output_dim=self.dim_vec,\n",
    "                              mask_zero=True,\n",
    "                              embeddings_initializer=keras_uniform(seed=12345)\n",
    "                             )(dec_input)\n",
    "        \n",
    "        tf_tensor = BatchNormalization(axis=-1)(tf_tensor)\n",
    "        dec_LSTM_input = Masking(mask_value=0.0)(tf_tensor)\n",
    "        \n",
    "        # dec_LSTM is used later again\n",
    "        dec_LSTM = LSTM(\n",
    "                        units=self.dim_hidden_vec,\n",
    "                        kernel_initializer=keras_glorot_uniform(seed=12345),\n",
    "                        recurrent_initializer=keras_orthogonal(gain=1.0, seed=12345),\n",
    "                        dropout=0.5,\n",
    "                        recurrent_dropout=0.5,\n",
    "                        return_state=True,\n",
    "                        return_sequences=True,\n",
    "                       )\n",
    "        tf_tensor, _, _ = dec_LSTM(dec_LSTM_input, initial_state=[enc_hidden_state, enc_cell_state])\n",
    "        \n",
    "        # Densely connected NN after LSTM is used later again\n",
    "        dec_Dense = Dense(\n",
    "                          units=self.num_output,\n",
    "                          activation='softmax',\n",
    "                          kernel_initializer=keras_glorot_uniform(seed=12345)\n",
    "                         ) \n",
    "        dec_output = dec_Dense(tf_tensor)\n",
    "        \n",
    "        model = Model(inputs=[enc_input, dec_input], outputs=dec_output)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['categorical_accuracy'])\n",
    "        \n",
    "        print('Decoder training model created.\\n')\n",
    "        \n",
    "        #\n",
    "        # Decoder model creation\n",
    "        #\n",
    "        \n",
    "        dec_hidd_state_input = Input(shape=(self.dim_hidden_vec,), name='hidd_state_input')\n",
    "        dec_cell_state_input = Input(shape=(self.dim_hidden_vec,), name='cell_state_input')\n",
    "        \n",
    "        \n",
    "        tf_tensor, dec_hidden_state, dec_cell_state = dec_LSTM(\n",
    "                                                                dec_LSTM_input, \n",
    "                                                                initial_state=[dec_hidd_state_input, dec_cell_state_input]\n",
    "                                                                )\n",
    "        dec_response = dec_Dense(tf_tensor)\n",
    "        \n",
    "        dec_model = Model(\n",
    "                          inputs =[dec_input, dec_hidd_state_input, dec_cell_state_input],\n",
    "                          outputs=[dec_response, dec_hidden_state, dec_cell_state]\n",
    "                         )\n",
    "        \n",
    "        print('Decoder model created.\\n')\n",
    "        \n",
    "        return model, enc_model, dec_model\n",
    "    \n",
    "        \n",
    "    \n",
    "    def eval_perplexity(self):     # Used in on_batch() and test\n",
    "        pass\n",
    "    \n",
    "    def on_batch(self):            # Used in train()\n",
    "        pass\n",
    "    \n",
    "    def train(self):\n",
    "        self.create_model()\n",
    "    \n",
    "    def sampling_detoknize(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Start creating models.\n",
      "\n",
      "Encoder model created.\n",
      "\n",
      "Decoder training model created.\n",
      "\n",
      "Decoder model created.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_input = len(words)\n",
    "num_output = num_input\n",
    "dim_vec = 400\n",
    "dim_hidden_vec = int(dim_vec * 2)\n",
    "\n",
    "dialog = Dialog(maxlen_e, maxlen_d, num_input, num_output, dim_vec, dim_hidden_vec)\n",
    "\n",
    "dialog.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
