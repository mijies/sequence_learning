{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import csv\n",
    "import glob\n",
    "import io\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to parse corpus files\n",
    "\n",
    "def parse_corpus(fname, corpus):\n",
    "    \n",
    "    f = open(fname, 'r', encoding='utf-8')\n",
    "    # File is non-csv format but comma \",\" does not exist.\n",
    "    # Csv reader is used to avoid text garbling.\n",
    "    iter_obj = csv.reader(f)\n",
    "    lines = [v for v in iter_obj]\n",
    "    f.close()\n",
    "    \n",
    "    text = ''\n",
    "    for line in lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "\n",
    "        s = line[0]\n",
    "        if len(s) == 0: continue\n",
    "\n",
    "        # ％ｃｏｍ： is supplementary info\n",
    "        if s[0:5] == \"％ｃｏｍ：\":\n",
    "#                 print(' - Line', i, ': skip ％ｃｏｍ')\n",
    "            continue\n",
    "\n",
    "        if s[0] == '＠':\n",
    "#                 print(' - Line', i, ': skip ＠ meta info ')\n",
    "            continue\n",
    "        else:\n",
    "            #Replace stars with UNK\n",
    "            s = s.replace('＊＊＊','UNK')\n",
    "            #Replace speaker symbols with a separater\n",
    "            if s[0] == 'F' or s[0] == 'M':\n",
    "                s = 'SSSS'+s[5:]\n",
    "            if s[0:2] == 'Ｘ：':\n",
    "                s = 'SSSS'+s[2:]\n",
    "\n",
    "            s = re.sub('F[0-9]{3}',\"UNK\",s)\n",
    "            s = re.sub('M[0-9]{3}',\"UNK\",s)\n",
    "            s = s.replace(\"＊\",\"\")\n",
    "\n",
    "\n",
    "        # ?? Should be after concatinating the values ??\n",
    "        for L, R in zip(['（', '＜', '【'], ['）', '＞', '】']):\n",
    "            while s.find(L) != -1:\n",
    "                left_pos = s.find(L)\n",
    "                if s.find(R) != -1:\n",
    "                    right_pos = s.find(R)\n",
    "                    if left_pos > right_pos:\n",
    "                        if s[0:4] == 'SSSS':\n",
    "                            s = s.replace(s[4:right_pos+1], '', 1)\n",
    "                        else:\n",
    "                            s = s.replace(s[:right_pos+1], '', 1)\n",
    "                    else:\n",
    "                        s = s.replace(s[left_pos:right_pos+1], '')\n",
    "                        if len(s) == 0:\n",
    "                            continue\n",
    "                else:\n",
    "                    s=s[0:left_pos]\n",
    "\n",
    "        if s != \"\\n\" and s != \"SSSS\":\n",
    "            text += s\n",
    "\n",
    "\n",
    "    if text[0:4] != 'SSSS':\n",
    "        text = 'SSSS' + text\n",
    "    while text[0:4] == 'SSSS':\n",
    "        next_pos = text[4:].find(\"SSSS\")\n",
    "        if next_pos == -1:\n",
    "            corpus.append(text)\n",
    "            break\n",
    "        else:\n",
    "            corpus.append(text[:4+next_pos])\n",
    "            text = text[4+next_pos:]\n",
    "        # Breaks up a long sentence\n",
    "        # ?? The split lines are considered a talk between 2. Any influence in training ??\n",
    "        if len(corpus[-1]) > 50:\n",
    "            xs = corpus[-1].split('。')\n",
    "            if len(xs) == 1:\n",
    "                continue\n",
    "            corpus.pop()\n",
    "            if len(xs[0]) > 30:\n",
    "                corpus.append(xs[0].split('、')[0] + '。')\n",
    "            else:\n",
    "                corpus.append(xs[0] + '。')\n",
    "            while xs[-1] == '' or xs[-1] == ' ' or xs[-1] == '　':\n",
    "                xs.pop()\n",
    "            if len(xs) > 1:\n",
    "                if len(xs[-1]) > 30:\n",
    "                    corpus.append('SSSS' + xs[-1].split('、')[-1] + '。')\n",
    "                else:\n",
    "                    corpus.append('SSSS' + xs[-1] + '。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Started parsing corpus files\n",
      "\n",
      "The number of the corpus files are 129 \n",
      "\n",
      "The number of the corpus is 83076 \n",
      "\n",
      "...completed parsing corpus into \"data/nucc_corpus.txt\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse corpus\n",
    "\n",
    "print('\\n...Started parsing corpus files\\n')\n",
    "\n",
    "fname_list = glob.glob('data/nucc/*')\n",
    "print('The number of the corpus files are', len(fname_list), '\\n')\n",
    "\n",
    "corpus = []\n",
    "for fname in fname_list:\n",
    "    parse_corpus(fname, corpus)\n",
    "print('The number of the corpus is', len(corpus), '\\n')\n",
    "\n",
    "with open('data/nucc_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in corpus:\n",
    "        f.write(line + \"\\n\")\n",
    "print('...completed parsing corpus into \"data/nucc_corpus.txt\"\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Started morphological analysis on \"data/nucc_corpus.txt\"\n",
      "\n",
      "...completed the analysis into \"data/nucc_corpus_analyzed.txt\"\n",
      "\n",
      "...extracting only tokens from \"data/nucc_corpus.txt\"\n",
      "\n",
      "...completed the extracting into \"data/nucc_tokens.txt\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Morphological analysis\n",
    "# !! Note this step can take more than minutes !!\n",
    "\n",
    "print('\\n...Started morphological analysis on \"data/nucc_corpus.txt\"\\n')\n",
    "!jumanpp -f < data/nucc_corpus.txt > data/nucc_corpus_analyzed.txt\n",
    "print('...completed the analysis into \"data/nucc_corpus_analyzed.txt\"\\n')\n",
    "\n",
    "print('...extracting only tokens from \"data/nucc_corpus.txt\"\\n')\n",
    "!cat data/nucc_corpus_analyzed.txt | cut -f1 -d\\  > data/nucc_tokens.txt\n",
    "print('...completed the extracting into \"data/nucc_tokens.txt\"\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to tokenize parsed corpus file\n",
    "\n",
    "def tokenize_corpus(src_fname, dst_fname):\n",
    "    \n",
    "    f = open(src_fname, 'r')\n",
    "    data = f.read()\n",
    "    f.close()\n",
    "    print('Input file is', src_fname.split('/')[-1], '\\n')\n",
    "    \n",
    "    print('The byte size of the input token is', len(data), '\\n')\n",
    "    data = re.sub('.*SSSS.*', 'SSSS', data)\n",
    "    data = re.sub('SSSS\\nSSSS', 'SSSS', data)\n",
    "    data = re.sub('.*UNK.*', 'UNK', data)\n",
    "    data = re.sub('@\\n', '', data)\n",
    "    data = re.sub('EOS\\n', '', data)\n",
    "\n",
    "    # File is non-csv format but comma \",\" does not exist.\n",
    "    # Csv reader is used to avoid text garbling.\n",
    "    iter_obj = csv.reader(io.StringIO(data))\n",
    "\n",
    "    token_list = [v for v in iter_obj]\n",
    "    print('The number of the input token is', len(token_list), '\\n')\n",
    "    token_list.append('SSSS')\n",
    "    token_array = np.array(token_list).reshape(len(token_list), 1) \n",
    "\n",
    "    print('The number of the output token is', token_array.shape, '\\n')\n",
    "    np.save(dst_fname, token_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Started tokenizing corpus file\n",
      "\n",
      "Input file is nucc_tokens.txt \n",
      "\n",
      "The byte size of the input token is 2837186 \n",
      "\n",
      "The number of the input token is 811177 \n",
      "\n",
      "The number of the output token is (811178, 1) \n",
      "\n",
      "...completed tokenizing into \"data/nucc_tokens.npy\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize corpus\n",
    "\n",
    "print('\\n...Started tokenizing corpus file\\n')\n",
    "\n",
    "tokenize_corpus('data/nucc_tokens.txt', 'data/nucc_tokens.npy')\n",
    "\n",
    "print('...completed tokenizing into \"data/nucc_tokens.npy\"\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter_obj = csv.reader(open('data/nucc_corpus_analyzed.txt', 'r'), delimiter=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "li = [v[0] for v in iter_obj] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat = [v[0] for v in li] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "935512"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
