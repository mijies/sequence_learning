{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from keras.initializers import glorot_uniform as keras_glorot_uniform\n",
    "from keras.initializers import orthogonal as keras_orthogonal\n",
    "from keras.initializers import uniform as keras_uniform\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Masking\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78881, 50, 1)\n",
      "(78881, 50)\n"
     ]
    }
   ],
   "source": [
    "# Prepeare data set\n",
    "\n",
    "with open('data/words.pickle', 'rb') as f:\n",
    "    words = pickle.load(f)\n",
    "    \n",
    "enc_input = np.load('data/enc_input.npy')\n",
    "dec_input = np.load('data/dec_input.npy')\n",
    "lbl_input = np.load('data/lbl_input.npy')\n",
    "maxlen_e, maxlen_d = np.load('data/maxlen.npy')\n",
    "\n",
    "\n",
    "# Split data for training and test\n",
    "sep_idx = int(enc_input.shape[0] * 0.95)\n",
    "enc_train, enc_test = np.vsplit(enc_input, [sep_idx])\n",
    "dec_train, dec_test = np.vsplit(dec_input, [sep_idx])\n",
    "lbl_train, lbl_test = np.vsplit(lbl_input, [sep_idx])\n",
    "\n",
    "print(enc_train.shape) # ?? 3rd dimension needed ??\n",
    "\n",
    "num = enc_train.shape[0]\n",
    "enc_train = enc_train.reshape(num, maxlen_e)\n",
    "dec_train = dec_train.reshape(num, maxlen_d)\n",
    "lbl_train = lbl_train.reshape(num, maxlen_d)\n",
    "\n",
    "print(enc_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dialog:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        maxlen_e,\n",
    "        maxlen_d,\n",
    "        num_input,\n",
    "        num_output,\n",
    "        dim_vec,\n",
    "        dim_hidden_vec\n",
    "    ):\n",
    "        self.maxlen_e = maxlen_e\n",
    "        self.maxlen_d = maxlen_d\n",
    "        self.num_input = num_input\n",
    "        self.num_output = num_output\n",
    "        self.dim_vec = dim_vec\n",
    "        self.dim_hidden_vec = dim_hidden_vec\n",
    "        \n",
    "        \n",
    "        \n",
    "    def create_model(self):        # Used in train() and prediction\n",
    "        \n",
    "        print('\\n...Start creating models.\\n')\n",
    "        \n",
    "        #\n",
    "        # Encoder model creation\n",
    "        #\n",
    "        \n",
    "        enc_input = Input(shape=(self.maxlen_e,), dtype='int32', name='encoder_input')\n",
    "        tf_tensor = Embedding(\n",
    "                              input_dim=self.num_input, \n",
    "                              output_dim=self.num_output, \n",
    "                              mask_zero=True, # ID 0 in Input data becomes a padding.(LSTM layer just passes the previous values)\n",
    "                              embeddings_initializer=keras_uniform(seed=12345)\n",
    "                             )(enc_input)\n",
    "        \n",
    "        # axis -1 counts dimension from the highest rank\n",
    "        tf_tensor = BatchNormalization(axis=-1)(tf_tensor)\n",
    "        \n",
    "        # When all IDs in a timestep is equal to mask_value, skipped in all downstream layers\n",
    "        # Maybe input data unlikely has a sequence whose all values are maske_value as corpus is parsed in such a way\n",
    "        tf_tensor = Masking(mask_value=0.0)(tf_tensor)\n",
    "        \n",
    "        enc_output, enc_hidden_state, enc_cell_state = LSTM(\n",
    "                                                            units=self.dim_hidden_vec,\n",
    "                                                            kernel_initializer=keras_glorot_uniform(seed=12345),\n",
    "                                                            # Coefficient to the orthogonal matrix is 1.0\n",
    "                                                            recurrent_initializer=keras_orthogonal(gain=1.0, seed=12345),\n",
    "                                                            dropout=0.5,\n",
    "                                                            recurrent_dropout=0.5,\n",
    "                                                            return_state=True,\n",
    "                                                           )(tf_tensor)\n",
    "        \n",
    "        enc_model  = Model(inputs=enc_input, outputs=[enc_output, enc_hidden_state, enc_cell_state])\n",
    "\n",
    "        print('Encoder model created.\\n')\n",
    "        \n",
    "        #\n",
    "        # Decoder training model creation\n",
    "        #\n",
    "        \n",
    "        dec_input = Input(shape=(self.maxlen_d,), dtype='int32', name='decoder_input')\n",
    "        tf_tensor = Embedding(\n",
    "                              input_dim=self.num_input,\n",
    "                              output_dim=self.dim_vec,\n",
    "                              mask_zero=True,\n",
    "                              embeddings_initializer=keras_uniform(seed=12345)\n",
    "                             )(dec_input)\n",
    "        \n",
    "        tf_tensor = BatchNormalization(axis=-1)(tf_tensor)\n",
    "        dec_LSTM_input = Masking(mask_value=0.0)(tf_tensor)\n",
    "        \n",
    "        # dec_LSTM is used later again\n",
    "        dec_LSTM = LSTM(\n",
    "                        units=self.dim_hidden_vec,\n",
    "                        kernel_initializer=keras_glorot_uniform(seed=12345),\n",
    "                        recurrent_initializer=keras_orthogonal(gain=1.0, seed=12345),\n",
    "                        dropout=0.5,\n",
    "                        recurrent_dropout=0.5,\n",
    "                        return_state=True,\n",
    "                        return_sequences=True,\n",
    "                       )\n",
    "        tf_tensor, _, _ = dec_LSTM(dec_LSTM_input, initial_state=[enc_hidden_state, enc_cell_state])\n",
    "        \n",
    "        # Densely connected NN after LSTM is used later again\n",
    "        dec_Dense = Dense(\n",
    "                          units=self.num_output,\n",
    "                          activation='softmax',\n",
    "                          kernel_initializer=keras_glorot_uniform(seed=12345)\n",
    "                         ) \n",
    "        dec_output = dec_Dense(tf_tensor)\n",
    "        \n",
    "        model = Model(inputs=[enc_input, dec_input], outputs=dec_output)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['categorical_accuracy'])\n",
    "        \n",
    "        print('Decoder training model created.\\n')\n",
    "        \n",
    "        #\n",
    "        # Decoder model creation\n",
    "        #\n",
    "        \n",
    "        dec_hidd_state_input = Input(shape=(self.dim_hidden_vec,), name='hidd_state_input')\n",
    "        dec_cell_state_input = Input(shape=(self.dim_hidden_vec,), name='cell_state_input')\n",
    "        \n",
    "        \n",
    "        tf_tensor, dec_hidden_state, dec_cell_state = dec_LSTM(\n",
    "                                                                dec_LSTM_input, \n",
    "                                                                initial_state=[dec_hidd_state_input, dec_cell_state_input]\n",
    "                                                                )\n",
    "        dec_response = dec_Dense(tf_tensor)\n",
    "        \n",
    "        dec_model = Model(\n",
    "                          inputs =[dec_input, dec_hidd_state_input, dec_cell_state_input],\n",
    "                          outputs=[dec_response, dec_hidden_state, dec_cell_state]\n",
    "                         )\n",
    "        \n",
    "        print('Decoder model created.\\n')\n",
    "        \n",
    "        return model, enc_model, dec_model\n",
    "    \n",
    "\n",
    "\n",
    "    def train(self, enc_train, dec_train, lbl_train, epochs, batch_size, param_file):\n",
    "\n",
    "        model, _, _ = self.create_model()\n",
    "\n",
    "        if os.path.isfile(param_file):\n",
    "            model.load_weights(param_file)\n",
    "\n",
    "        for v in [enc_train, dec_train, lbl_train]:\n",
    "            np.random.seed(12345)\n",
    "            np.random.shuffle(v)\n",
    "            \n",
    "        \n",
    "        # Split data for training and validation\n",
    "        sep_idx = int(enc_train.shape[0] * 0.9)\n",
    "        enc_train, enc_validate = np.vsplit(enc_input, [sep_idx])\n",
    "        dec_train, dec_validate = np.vsplit(dec_input, [sep_idx])\n",
    "        lbl_train, lbl_validate = np.vsplit(lbl_input, [sep_idx])\n",
    "        \n",
    "        \n",
    "        # ?? Dimension changed from (N, maxlen) (N, maxlen, 1) ??\n",
    "        enc_train    = enc_train.reshape(enc_train.shape[0], self.maxlen_e)\n",
    "        enc_validate = enc_validate.reshape(enc_validate.shape[0], self.maxlen_e)\n",
    "        dec_train    = dec_train.reshape(dec_train.shape[0], self.maxlen_d)\n",
    "        dec_validate = dec_validate.reshape(dec_validate.shape[0], self.maxlen_d)\n",
    "        lbl_train    = lbl_train.reshape(lbl_train.shape[0], self.maxlen_d)\n",
    "        lbl_validate = lbl_validate.reshape(lbl_validate.shape[0], self.maxlen_d)\n",
    "        \n",
    "\n",
    "        early_stop_cap = 10000\n",
    "        \n",
    "        for epoch_idx in range(0, epochs):\n",
    "            \n",
    "            print('Epoch ', epoch_idx+1, '/', epochs) \n",
    "            validate_perplexity = self.on_batch(\n",
    "                                                model, \n",
    "                                                epoch_idx, \n",
    "                                                enc_train,\n",
    "                                                dec_train,\n",
    "                                                lbl_train,\n",
    "                                                enc_validate,\n",
    "                                                dec_validate,\n",
    "                                                lbl_validate,\n",
    "                                                batch_size\n",
    "                                               )\n",
    "            #EarlyStopping\n",
    "            if epoch_idx == 0 or validate_perplexity <= early_stop_cap:\n",
    "                  early_stop_cap = validate_perplexity\n",
    "            else:\n",
    "                print('Early stopping\\n') \n",
    "                break\n",
    "                  \n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def on_batch(self,     # Used in train()\n",
    "         model, \n",
    "         epoch_idx, \n",
    "         enc_train,\n",
    "         dec_train,\n",
    "         lbl_train,\n",
    "         enc_validate,\n",
    "         dec_validate,\n",
    "         lbl_validate,\n",
    "         batch_size\n",
    "    ):\n",
    "                 \n",
    "        list_loss = []\n",
    "        list_acc  = []\n",
    "        num_train = enc_train.shape[0]\n",
    "\n",
    "        time_start = time.time()\n",
    "        \n",
    "        for step in range(0, math.ceil(num_train / batch_size)):\n",
    "            if step > 0:break\n",
    "            \n",
    "            s = batch_size * step\n",
    "            e = min(num_train, batch_size * (step + 1))\n",
    "            \n",
    "            enc_batch = enc_train[s:e, :]\n",
    "            dec_batch = dec_train[s:e, :]\n",
    "            lbl_batch = lbl_train[s:e, :]\n",
    "\n",
    "            \n",
    "            # turns IDs into binary values\n",
    "            lbl_batch = np_utils.to_categorical(lbl_batch, self.num_output)\n",
    "            \n",
    "            # train_on_batch returns [Loss scalar, Accuracy scalar]\n",
    "            result = model.train_on_batch([enc_batch, dec_batch], lbl_batch)\n",
    "            \n",
    "            list_loss.append(result[0])\n",
    "            list_acc.append(result[1])\n",
    "            \n",
    "            print(\n",
    "                  '\\n', e, '/', num_train, '\\t', str(int(time.time() - time_start)) + 'sec',\n",
    "                  '\\t', 'Loss :', \"{0:.4f}\".format(np.average(list_loss)),\n",
    "                  '\\t', 'Acc :',  \"{0:.4f}\".format(np.average(list_acc))\n",
    "                 )\n",
    "            \n",
    "            del enc_batch, dec_batch, lbl_batch\n",
    "        \n",
    "        del list_loss, list_acc\n",
    "        \n",
    "        return self.eval_perplexity(\n",
    "                    model,\n",
    "                    enc_validate,\n",
    "                    dec_validate,\n",
    "                    lbl_validate,\n",
    "                    batch_size\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "    def eval_perplexity(self,    # Used in on_batch() and test\n",
    "        model,\n",
    "        enc_validate,\n",
    "        dec_validate,\n",
    "        lbl_validate,\n",
    "        batch_size\n",
    "    ):\n",
    "        \n",
    "        list_loss = []\n",
    "        num_validate = enc_validate.shape[0]\n",
    "        \n",
    "        num_loss = 0\n",
    "        sum_loss = 0\n",
    "\n",
    "        time_start = time.time()\n",
    "        \n",
    "        for step in range(0, math.ceil(num_validate / batch_size)):\n",
    "            if step > 1:break\n",
    "            \n",
    "            s = batch_size * step\n",
    "            e = min(num_validate, batch_size * (step + 1))\n",
    "            \n",
    "            enc_batch = enc_validate[s:e, :]\n",
    "            dec_batch = dec_validate[s:e, :]\n",
    "            lbl_batch = lbl_validate[s:e, :]\n",
    "\n",
    "            \n",
    "            # turns IDs into binary values\n",
    "            lbl_batch = np_utils.to_categorical(lbl_batch, self.num_output)\n",
    "            \n",
    "            # Reshape the dimension into (1, N * maxlen) for cross entropy Loss dot product\n",
    "            lbl_batch = lbl_batch.reshape(1,(e-s) * self.maxlen_d * self.num_output)\n",
    "            \n",
    "# #             mask = np.zeros((e-s, self.maxlen_d, self.num_output), dtype=np.float32)\n",
    "# #             print(mask.shape)\n",
    "\n",
    "            for i in range(0,e-s):\n",
    "                num_dim = self.maxlen_d - list(dec_batch[i, :]).count(0.)\n",
    "                num_loss += num_dim   # Only take the number of tokens(IDs) excluding the paddings\n",
    "\n",
    "            Y1 = model.predict_on_batch([enc_batch, dec_batch])\n",
    "            Y2 = np.maximum(Y1, 1e-7)   # Replace the values less than 1e-7 with 1e-7\n",
    "            Y2 = np.log(Y2)\n",
    "            Y3 = Y2.reshape(1, (e-s) * self.maxlen_d * self.num_output)\n",
    "            \n",
    "            loss = np.dot(Y3, lbl_batch.T) # Shape (1, 1)\n",
    "            sum_loss += loss[0, 0]\n",
    "            \n",
    "            perplexity = np.exp(sum_loss / num_loss)\n",
    "            \n",
    "            print(\n",
    "                  '\\n', e, '/', num_validate, '\\t', str(int(time.time() - time_start)) + 'sec',\n",
    "                  '\\t', 'Perplexity :',  \"{0:.4f}\".format(perplexity)\n",
    "                 )\n",
    "            \n",
    "            del enc_batch, dec_batch, lbl_batch\n",
    "            del Y1, Y2, Y3\n",
    "            gc.collect()\n",
    "        \n",
    "        return 10000\n",
    "    \n",
    "        \n",
    "    \n",
    "    def sampling_detoknize(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Start creating models.\n",
      "\n",
      "Encoder model created.\n",
      "\n",
      "Decoder training model created.\n",
      "\n",
      "Decoder model created.\n",
      "\n",
      "Epoch  1 / 1\n",
      "\n",
      " 100 / 70992 \t 67sec \t Loss : 8.8770 \t Acc : 0.0000\n",
      "\n",
      " 100 / 12041 \t 14sec \t Perplexity : 0.0000\n",
      "\n",
      " 200 / 12041 \t 28sec \t Perplexity : 0.0000\n",
      "\n",
      " 300 / 12041 \t 39sec \t Perplexity : 0.0000\n",
      "\n",
      " 400 / 12041 \t 50sec \t Perplexity : 0.0000\n",
      "\n",
      " 500 / 12041 \t 62sec \t Perplexity : 0.0000\n",
      "\n",
      " 600 / 12041 \t 73sec \t Perplexity : 0.0000\n",
      "\n",
      " 700 / 12041 \t 85sec \t Perplexity : 0.0000\n",
      "\n",
      " 800 / 12041 \t 96sec \t Perplexity : 0.0000\n",
      "\n",
      " 900 / 12041 \t 108sec \t Perplexity : 0.0000\n",
      "\n",
      " 1000 / 12041 \t 119sec \t Perplexity : 0.0000\n",
      "\n",
      " 1100 / 12041 \t 130sec \t Perplexity : 0.0000\n",
      "\n",
      " 1200 / 12041 \t 141sec \t Perplexity : 0.0000\n",
      "\n",
      " 1300 / 12041 \t 153sec \t Perplexity : 0.0000\n",
      "\n",
      " 1400 / 12041 \t 165sec \t Perplexity : 0.0000\n",
      "\n",
      " 1500 / 12041 \t 177sec \t Perplexity : 0.0000\n",
      "\n",
      " 1600 / 12041 \t 188sec \t Perplexity : 0.0000\n",
      "\n",
      " 1700 / 12041 \t 200sec \t Perplexity : 0.0000\n",
      "\n",
      " 1800 / 12041 \t 212sec \t Perplexity : 0.0000\n",
      "\n",
      " 1900 / 12041 \t 224sec \t Perplexity : 0.0000\n",
      "\n",
      " 2000 / 12041 \t 236sec \t Perplexity : 0.0000\n",
      "\n",
      " 2100 / 12041 \t 247sec \t Perplexity : 0.0000\n",
      "\n",
      " 2200 / 12041 \t 259sec \t Perplexity : 0.0000\n",
      "\n",
      " 2300 / 12041 \t 270sec \t Perplexity : 0.0000\n",
      "\n",
      " 2400 / 12041 \t 282sec \t Perplexity : 0.0000\n",
      "\n",
      " 2500 / 12041 \t 293sec \t Perplexity : 0.0000\n",
      "\n",
      " 2600 / 12041 \t 304sec \t Perplexity : 0.0000\n",
      "\n",
      " 2700 / 12041 \t 315sec \t Perplexity : 0.0000\n",
      "\n",
      " 2800 / 12041 \t 326sec \t Perplexity : 0.0000\n",
      "\n",
      " 2900 / 12041 \t 337sec \t Perplexity : 0.0000\n",
      "\n",
      " 3000 / 12041 \t 349sec \t Perplexity : 0.0000\n",
      "\n",
      " 3100 / 12041 \t 360sec \t Perplexity : 0.0000\n",
      "\n",
      " 3200 / 12041 \t 371sec \t Perplexity : 0.0000\n",
      "\n",
      " 3300 / 12041 \t 382sec \t Perplexity : 0.0000\n",
      "\n",
      " 3400 / 12041 \t 394sec \t Perplexity : 0.0000\n",
      "\n",
      " 3500 / 12041 \t 405sec \t Perplexity : 0.0000\n",
      "\n",
      " 3600 / 12041 \t 416sec \t Perplexity : 0.0000\n",
      "\n",
      " 3700 / 12041 \t 428sec \t Perplexity : 0.0000\n",
      "\n",
      " 3800 / 12041 \t 439sec \t Perplexity : 0.0000\n",
      "\n",
      " 3900 / 12041 \t 450sec \t Perplexity : 0.0000\n",
      "\n",
      " 4000 / 12041 \t 461sec \t Perplexity : 0.0000\n",
      "\n",
      " 4100 / 12041 \t 472sec \t Perplexity : 0.0000\n",
      "\n",
      " 4200 / 12041 \t 483sec \t Perplexity : 0.0000\n",
      "\n",
      " 4300 / 12041 \t 494sec \t Perplexity : 0.0000\n",
      "\n",
      " 4400 / 12041 \t 505sec \t Perplexity : 0.0000\n",
      "\n",
      " 4500 / 12041 \t 517sec \t Perplexity : 0.0000\n",
      "\n",
      " 4600 / 12041 \t 528sec \t Perplexity : 0.0000\n",
      "\n",
      " 4700 / 12041 \t 539sec \t Perplexity : 0.0000\n",
      "\n",
      " 4800 / 12041 \t 550sec \t Perplexity : 0.0000\n",
      "\n",
      " 4900 / 12041 \t 561sec \t Perplexity : 0.0000\n",
      "\n",
      " 5000 / 12041 \t 573sec \t Perplexity : 0.0000\n",
      "\n",
      " 5100 / 12041 \t 584sec \t Perplexity : 0.0000\n",
      "\n",
      " 5200 / 12041 \t 595sec \t Perplexity : 0.0000\n",
      "\n",
      " 5300 / 12041 \t 606sec \t Perplexity : 0.0000\n",
      "\n",
      " 5400 / 12041 \t 617sec \t Perplexity : 0.0000\n",
      "\n",
      " 5500 / 12041 \t 629sec \t Perplexity : 0.0000\n",
      "\n",
      " 5600 / 12041 \t 640sec \t Perplexity : 0.0000\n",
      "\n",
      " 5700 / 12041 \t 651sec \t Perplexity : 0.0000\n",
      "\n",
      " 5800 / 12041 \t 662sec \t Perplexity : 0.0000\n",
      "\n",
      " 5900 / 12041 \t 673sec \t Perplexity : 0.0000\n",
      "\n",
      " 6000 / 12041 \t 685sec \t Perplexity : 0.0000\n",
      "\n",
      " 6100 / 12041 \t 696sec \t Perplexity : 0.0000\n",
      "\n",
      " 6200 / 12041 \t 707sec \t Perplexity : 0.0000\n",
      "\n",
      " 6300 / 12041 \t 718sec \t Perplexity : 0.0000\n",
      "\n",
      " 6400 / 12041 \t 730sec \t Perplexity : 0.0000\n",
      "\n",
      " 6500 / 12041 \t 741sec \t Perplexity : 0.0000\n",
      "\n",
      " 6600 / 12041 \t 752sec \t Perplexity : 0.0000\n",
      "\n",
      " 6700 / 12041 \t 763sec \t Perplexity : 0.0000\n",
      "\n",
      " 6800 / 12041 \t 774sec \t Perplexity : 0.0000\n",
      "\n",
      " 6900 / 12041 \t 786sec \t Perplexity : 0.0000\n",
      "\n",
      " 7000 / 12041 \t 797sec \t Perplexity : 0.0000\n",
      "\n",
      " 7100 / 12041 \t 808sec \t Perplexity : 0.0000\n",
      "\n",
      " 7200 / 12041 \t 820sec \t Perplexity : 0.0000\n",
      "\n",
      " 7300 / 12041 \t 831sec \t Perplexity : 0.0000\n",
      "\n",
      " 7400 / 12041 \t 842sec \t Perplexity : 0.0000\n",
      "\n",
      " 7500 / 12041 \t 854sec \t Perplexity : 0.0000\n",
      "\n",
      " 7600 / 12041 \t 865sec \t Perplexity : 0.0000\n",
      "\n",
      " 7700 / 12041 \t 876sec \t Perplexity : 0.0000\n",
      "\n",
      " 7800 / 12041 \t 887sec \t Perplexity : 0.0000\n",
      "\n",
      " 7900 / 12041 \t 898sec \t Perplexity : 0.0000\n",
      "\n",
      " 8000 / 12041 \t 910sec \t Perplexity : 0.0000\n",
      "\n",
      " 8100 / 12041 \t 921sec \t Perplexity : 0.0000\n",
      "\n",
      " 8200 / 12041 \t 933sec \t Perplexity : 0.0000\n",
      "\n",
      " 8300 / 12041 \t 944sec \t Perplexity : 0.0000\n",
      "\n",
      " 8400 / 12041 \t 955sec \t Perplexity : 0.0000\n",
      "\n",
      " 8500 / 12041 \t 967sec \t Perplexity : 0.0000\n",
      "\n",
      " 8600 / 12041 \t 978sec \t Perplexity : 0.0000\n",
      "\n",
      " 8700 / 12041 \t 989sec \t Perplexity : 0.0000\n",
      "\n",
      " 8800 / 12041 \t 1001sec \t Perplexity : 0.0000\n",
      "\n",
      " 8900 / 12041 \t 1012sec \t Perplexity : 0.0000\n",
      "\n",
      " 9000 / 12041 \t 1023sec \t Perplexity : 0.0000\n",
      "\n",
      " 9100 / 12041 \t 1035sec \t Perplexity : 0.0000\n",
      "\n",
      " 9200 / 12041 \t 1046sec \t Perplexity : 0.0000\n",
      "\n",
      " 9300 / 12041 \t 1057sec \t Perplexity : 0.0000\n",
      "\n",
      " 9400 / 12041 \t 1069sec \t Perplexity : 0.0000\n",
      "\n",
      " 9500 / 12041 \t 1080sec \t Perplexity : 0.0000\n",
      "\n",
      " 9600 / 12041 \t 1091sec \t Perplexity : 0.0000\n",
      "\n",
      " 9700 / 12041 \t 1103sec \t Perplexity : 0.0000\n",
      "\n",
      " 9800 / 12041 \t 1114sec \t Perplexity : 0.0000\n",
      "\n",
      " 9900 / 12041 \t 1126sec \t Perplexity : 0.0000\n",
      "\n",
      " 10000 / 12041 \t 1137sec \t Perplexity : 0.0000\n",
      "\n",
      " 10100 / 12041 \t 1148sec \t Perplexity : 0.0000\n",
      "\n",
      " 10200 / 12041 \t 1160sec \t Perplexity : 0.0000\n",
      "\n",
      " 10300 / 12041 \t 1172sec \t Perplexity : 0.0000\n",
      "\n",
      " 10400 / 12041 \t 1184sec \t Perplexity : 0.0000\n",
      "\n",
      " 10500 / 12041 \t 1196sec \t Perplexity : 0.0000\n",
      "\n",
      " 10600 / 12041 \t 1208sec \t Perplexity : 0.0000\n",
      "\n",
      " 10700 / 12041 \t 1219sec \t Perplexity : 0.0000\n",
      "\n",
      " 10800 / 12041 \t 1230sec \t Perplexity : 0.0000\n",
      "\n",
      " 10900 / 12041 \t 1242sec \t Perplexity : 0.0000\n",
      "\n",
      " 11000 / 12041 \t 1254sec \t Perplexity : 0.0000\n",
      "\n",
      " 11100 / 12041 \t 1265sec \t Perplexity : 0.0000\n",
      "\n",
      " 11200 / 12041 \t 1277sec \t Perplexity : 0.0000\n",
      "\n",
      " 11300 / 12041 \t 1288sec \t Perplexity : 0.0000\n",
      "\n",
      " 11400 / 12041 \t 1300sec \t Perplexity : 0.0000\n",
      "\n",
      " 11500 / 12041 \t 1311sec \t Perplexity : 0.0000\n",
      "\n",
      " 11600 / 12041 \t 1322sec \t Perplexity : 0.0000\n",
      "\n",
      " 11700 / 12041 \t 1334sec \t Perplexity : 0.0000\n",
      "\n",
      " 11800 / 12041 \t 1345sec \t Perplexity : 0.0000\n",
      "\n",
      " 11900 / 12041 \t 1356sec \t Perplexity : 0.0000\n",
      "\n",
      " 12000 / 12041 \t 1367sec \t Perplexity : 0.0000\n",
      "\n",
      " 12041 / 12041 \t 1373sec \t Perplexity : 0.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 100\n",
    "param_file = 'param_seq2seq.hdf5'\n",
    "\n",
    "num_input = len(words)\n",
    "num_output = num_input\n",
    "dim_vec = 400\n",
    "dim_hidden_vec = int(dim_vec * 2)\n",
    "\n",
    "\n",
    "prediction = Dialog(maxlen_e, maxlen_d, num_input, num_output, dim_vec, dim_hidden_vec)\n",
    "\n",
    "model = prediction.train(enc_train, dec_train, lbl_train, epochs, batch_size, param_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
