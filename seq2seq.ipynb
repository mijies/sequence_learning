{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import csv\n",
    "import glob\n",
    "import io\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to parse corpus files\n",
    "\n",
    "def parse_corpus(fname, corpus):\n",
    "    \n",
    "    f = open(fname, 'r', encoding='utf-8')\n",
    "    # File is non-csv format but comma \",\" does not exist.\n",
    "    # Csv reader is used to avoid text garbling.\n",
    "    iter_obj = csv.reader(f)\n",
    "    lines = [v for v in iter_obj]\n",
    "    f.close()\n",
    "    \n",
    "    text = ''\n",
    "    for line in lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "\n",
    "        s = line[0]\n",
    "        if len(s) == 0: continue\n",
    "\n",
    "        # ％ｃｏｍ： is supplementary info\n",
    "        if s[0:5] == \"％ｃｏｍ：\":\n",
    "#                 print(' - Line', i, ': skip ％ｃｏｍ')\n",
    "            continue\n",
    "\n",
    "        if s[0] == '＠':\n",
    "#                 print(' - Line', i, ': skip ＠ meta info ')\n",
    "            continue\n",
    "        else:\n",
    "            #Replace stars with UNK\n",
    "            s = s.replace('＊＊＊','UNK')\n",
    "            #Replace speaker symbols with a separater\n",
    "            if s[0] == 'F' or s[0] == 'M':\n",
    "                s = 'SSSS'+s[5:]\n",
    "            if s[0:2] == 'Ｘ：':\n",
    "                s = 'SSSS'+s[2:]\n",
    "\n",
    "            s = re.sub('F[0-9]{3}',\"UNK\",s)\n",
    "            s = re.sub('M[0-9]{3}',\"UNK\",s)\n",
    "            s = s.replace(\"＊\",\"\")\n",
    "\n",
    "\n",
    "        # ?? Should be after concatinating the values ??\n",
    "        for L, R in zip(['（', '＜', '【'], ['）', '＞', '】']):\n",
    "            while s.find(L) != -1:\n",
    "                left_pos = s.find(L)\n",
    "                if s.find(R) != -1:\n",
    "                    right_pos = s.find(R)\n",
    "                    if left_pos > right_pos:\n",
    "                        if s[0:4] == 'SSSS':\n",
    "                            s = s.replace(s[4:right_pos+1], '', 1)\n",
    "                        else:\n",
    "                            s = s.replace(s[:right_pos+1], '', 1)\n",
    "                    else:\n",
    "                        s = s.replace(s[left_pos:right_pos+1], '')\n",
    "                        if len(s) == 0:\n",
    "                            continue\n",
    "                else:\n",
    "                    s=s[0:left_pos]\n",
    "\n",
    "        if s != \"\\n\" and s != \"SSSS\":\n",
    "            text += s\n",
    "\n",
    "\n",
    "    if text[0:4] != 'SSSS':\n",
    "        text = 'SSSS' + text\n",
    "    while text[0:4] == 'SSSS':\n",
    "        next_pos = text[4:].find(\"SSSS\")\n",
    "        if next_pos == -1:\n",
    "            corpus.append(text)\n",
    "            break\n",
    "        else:\n",
    "            corpus.append(text[:4+next_pos])\n",
    "            text = text[4+next_pos:]\n",
    "        # Breaks up a long sentence\n",
    "        # ?? The split lines are considered a talk between 2. Any influence in training ??\n",
    "        if len(corpus[-1]) > 50:\n",
    "            xs = corpus[-1].split('。')\n",
    "            if len(xs) == 1:\n",
    "                continue\n",
    "            corpus.pop()\n",
    "            if len(xs[0]) > 30:\n",
    "                corpus.append(xs[0].split('、')[0] + '。')\n",
    "            else:\n",
    "                corpus.append(xs[0] + '。')\n",
    "            while xs[-1] == '' or xs[-1] == ' ' or xs[-1] == '　':\n",
    "                xs.pop()\n",
    "            if len(xs) > 1:\n",
    "                if len(xs[-1]) > 30:\n",
    "                    corpus.append('SSSS' + xs[-1].split('、')[-1] + '。')\n",
    "                else:\n",
    "                    corpus.append('SSSS' + xs[-1] + '。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Started parsing corpus files\n",
      "\n",
      "The number of the corpus files are 129 \n",
      "\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "The number of the corpus is 83076 \n",
      "\n",
      "...completed parsing corpus into \"data/nucc_corpus.txt\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse corpus\n",
    "\n",
    "print('\\n...Started parsing corpus files\\n')\n",
    "\n",
    "fname_list = glob.glob('data/nucc/*')\n",
    "print('The number of the corpus files are', len(fname_list), '\\n')\n",
    "\n",
    "corpus = []\n",
    "for fname in fname_list:\n",
    "    parse_corpus(fname, corpus)\n",
    "print('The number of the corpus is', len(corpus), '\\n')\n",
    "\n",
    "with open('data/nucc_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in corpus:\n",
    "        f.write(line + \"\\n\")\n",
    "print('...completed parsing corpus into \"data/nucc_corpus.txt\"\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Started morphological analysis on \"data/nucc_corpus.txt\"\n",
      "\n",
      "...completed the analysis into \"data/nucc_corpus_analyzed.txt\"\n",
      "\n",
      "...extracting only tokens from \"data/nucc_corpus.txt\"\n",
      "\n",
      "...completed the extracting into \"data/nucc_tokens.txt\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Morphological analysis\n",
    "# !! Note this step can take more than minutes !!\n",
    "\n",
    "print('\\n...Started morphological analysis on \"data/nucc_corpus.txt\"\\n')\n",
    "!jumanpp -f < data/nucc_corpus.txt > data/nucc_corpus_analyzed.txt\n",
    "print('...completed the analysis into \"data/nucc_corpus_analyzed.txt\"\\n')\n",
    "\n",
    "print('...extracting only tokens from \"data/nucc_corpus.txt\"\\n')\n",
    "!cat data/nucc_corpus_analyzed.txt | cut -f1 -d\\  > data/nucc_tokens.txt\n",
    "print('...completed the extracting into \"data/nucc_tokens.txt\"\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to tokenize parsed corpus file\n",
    "\n",
    "def tokenize_corpus(src_fname, dst_fname):\n",
    "    \n",
    "    f = open(src_fname, 'r')\n",
    "    data = f.read()\n",
    "    f.close()\n",
    "    print('Input file is', src_fname.split('/')[-1], '\\n')\n",
    "    \n",
    "    # ?? white_space and some signs are counted as tokens(should be ommited?) -> check word_index and index_word ??\n",
    "    print('The byte size of the input token is', len(data), '\\n')\n",
    "    data = re.sub('.*SSSS.*', 'SSSS', data)\n",
    "    data = re.sub('SSSS\\nSSSS', 'SSSS', data)\n",
    "    data = re.sub('.*UNK.*', 'UNK', data)\n",
    "    data = re.sub('@\\n', '', data)\n",
    "    data = re.sub('EOS\\n', '', data)\n",
    "\n",
    "    # File is non-csv format but comma \",\" does not exist.\n",
    "    # Csv reader is used to avoid text garbling.\n",
    "    iter_obj = csv.reader(io.StringIO(data))\n",
    "\n",
    "    token_list = []\n",
    "    for line in iter_obj:\n",
    "        if len(line) == 0: # You will face errors later with 0 length values\n",
    "            continue\n",
    "        token_list.append(line[0])\n",
    "    \n",
    "    token_array = np.array(token_list)\n",
    "    print('The shape of the output token(numpy array) is', token_array.shape, '\\n')\n",
    "\n",
    "    np.save(dst_fname, token_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Started tokenizing corpus file\n",
      "\n",
      "Input file is nucc_tokens.txt \n",
      "\n",
      "The byte size of the input token is 2837186 \n",
      "\n",
      "The shape of the output token(numpy array) is (811176,) \n",
      "\n",
      "...completed tokenizing into \"data/nucc_tokens.npy\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize corpus\n",
    "\n",
    "print('\\n...Started tokenizing corpus file\\n')\n",
    "\n",
    "tokenize_corpus('data/nucc_tokens.txt', 'data/nucc_tokens.npy')\n",
    "\n",
    "print('...completed tokenizing into \"data/nucc_tokens.npy\"\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create word/index dictionaries for training\n",
    "\n",
    "def make_word_index_dict(src_fname):\n",
    "    \n",
    "    token_array = np.load(src_fname)\n",
    "    words_uniq = sorted(set(token_array))\n",
    "    cnt = np.zeros(len(words_uniq))\n",
    "    \n",
    "    print('The number of the words is', len(words_uniq), '\\n')\n",
    "    \n",
    "    word_index = dict((w, i) for i, w in enumerate(words_uniq))\n",
    "    \n",
    "    for v in token_array:\n",
    "        cnt[word_index[v]] += 1\n",
    "    \n",
    "    to_unk_list = []\n",
    "    for i, c in enumerate(cnt): # Replace less frequent words with 'UNK'\n",
    "        if c < 4:\n",
    "            to_unk_list.append(words_uniq[i])\n",
    "            words_uniq[i] = 'UNK'\n",
    "    \n",
    "    print('The number of the words replaced with \"UNK\" is', len(to_unk_list), '\\n')\n",
    "    \n",
    "    # ?? Keras mask_zero=True can solve this. In need of more investigation ??\n",
    "    words_uniq.append('\\t') # \\t comes first by sorting, to keep index 0 for RNN masking(for 0 padding issue?)\n",
    "    words_uniq = sorted(set(words_uniq))\n",
    "    \n",
    "    print('The number of the words after UNK replacement is', len(words_uniq), '\\n')\n",
    "    \n",
    "    word_index = dict((w, i) for i, w in enumerate(words_uniq))\n",
    "    index_word = dict((i, w) for i, w in enumerate(words_uniq))\n",
    "\n",
    "    # !! Note this step can take more than minutes !!\n",
    "    token_index = np.zeros(len(token_array), dtype=int)\n",
    "    for i, v in enumerate(token_array):\n",
    "        if v in words_uniq:\n",
    "            token_index[i] = word_index[v]\n",
    "        else:\n",
    "            token_index[i] = word_index['UNK']\n",
    "    \n",
    "    with open('data/word_index.pickle', 'wb') as f :\n",
    "        pickle.dump(word_index , f)\n",
    "    print('Created \"data/word_index.pickle\"\\n')\n",
    "\n",
    "    with open('data/index_word.pickle', 'wb') as f :\n",
    "        pickle.dump(index_word , f)\n",
    "    print('Created \"data/index_word.pickle\"\\n')\n",
    "\n",
    "    with open('data/words.pickle', 'wb') as f :\n",
    "        pickle.dump(words_uniq , f)\n",
    "    print('Created \"data/words.pickle\"\\n')\n",
    "\n",
    "    with open('data/token_index.pickle', 'wb') as f :\n",
    "        pickle.dump(token_index , f)\n",
    "    print('Created \"data/token_index.pickle\"\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Started making dictionaries\n",
      "\n",
      "The number of the words is 22580 \n",
      "\n",
      "The number of the words replaced with \"UNK\" is 15388 \n",
      "\n",
      "The number of the words after UNK replacement is 7193 \n",
      "\n",
      "Created \"data/word_index.pickle\"\n",
      "\n",
      "Created \"data/index_word.pickle\"\n",
      "\n",
      "Created \"data/words.pickle\"\n",
      "\n",
      "Created \"data/token_index.pickle\"\n",
      "\n",
      "...completed making dictionaries\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create word/index dictionaries for training\n",
    "\n",
    "print('\\n...Started making dictionaries\\n')\n",
    "\n",
    "make_word_index_dict('data/nucc_tokens.npy')\n",
    "\n",
    "print('...completed making dictionaries\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create Encoder Input、Decoder Input and Labels for training\n",
    "\n",
    "def make_training_dataset(src_fname):\n",
    "    \n",
    "    maxlen_e = 50 # Maximal encoder input word number\n",
    "    maxlen_d = 50 # Maximal decoder input word number\n",
    "    \n",
    "    f = open('data/word_index.pickle', 'rb')\n",
    "    word_index = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    f = open('data/token_index.pickle', 'rb')\n",
    "    token_index = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "    #\n",
    "    # Convert corpus into dialog lists for input data\n",
    "    #\n",
    "\n",
    "    sep_idx = word_index['SSSS']\n",
    "    dialog_list = []\n",
    "\n",
    "    # ?? juman++ is not perfectly acurate we can check dialog_list then fix the original corpus ??\n",
    "    for i, idx in enumerate(token_index):\n",
    "        if idx == sep_idx:\n",
    "            if i != 0:\n",
    "                dialog_list.append(dialog)\n",
    "            dialog = []\n",
    "        else:\n",
    "            dialog.append(idx)\n",
    "\n",
    "    print('The number of the dialog is', len(dialog_list), '\\n')\n",
    "\n",
    "    \n",
    "    # Shuffle data\n",
    "    np.random.seed(12345)\n",
    "    np.random.shuffle(dialog_list)\n",
    "    \n",
    "    \n",
    "    enc_dialog = dialog_list[:-1]\n",
    "    dec_dialog = [[sep_idx] + v for v in dialog_list[1:]] # Insert 'SSSS' index at the head of dialog\n",
    "    lbl_dialog = [v + [sep_idx] for v in dialog_list[1:]] # Insert 'SSSS' index at the tail of dialog\n",
    "    \n",
    "    enc_input = []\n",
    "    dec_input = []\n",
    "    lbl_input = []\n",
    "\n",
    "    \n",
    "    # Keep dialogs only upto maxlen\n",
    "    for enc, dec, lbl in zip(enc_dialog, dec_dialog, lbl_dialog):\n",
    "        if len(enc) <= maxlen_e and len(dec) <= maxlen_d:\n",
    "            enc_input.append(enc)\n",
    "            dec_input.append(dec)\n",
    "            lbl_input.append(lbl)\n",
    "\n",
    "\n",
    "    # 0 padding upto maxlen\n",
    "    for i in range(0, len(enc_input)):\n",
    "        \n",
    "        # Extend each length with 0\n",
    "        enc_input[i] += [0] * maxlen_e\n",
    "        dec_input[i] += [0] * maxlen_d\n",
    "        lbl_input[i] += [0] * maxlen_d\n",
    "\n",
    "        # Cut off by the length of maxlen\n",
    "        enc_input[i] = enc_input[i][:maxlen_e]\n",
    "        dec_input[i] = dec_input[i][:maxlen_d]\n",
    "        lbl_input[i] = lbl_input[i][:maxlen_d]\n",
    "        \n",
    "    print('Each data was 0 padded upto maxlen\\n')\n",
    "    \n",
    "    enc_input = np.array(enc_input).reshape(len(enc_input), maxlen_e, 1)\n",
    "    dec_input = np.array(dec_input).reshape(len(dec_input), maxlen_d, 1)\n",
    "    lbl_input = np.array(lbl_input).reshape(len(lbl_input), maxlen_d, 1)\n",
    "    \n",
    "    print('The shape of Encoder Input is', enc_input.shape, '\\n')\n",
    "    print('The shape of Decoder Input is', dec_input.shape, '\\n')\n",
    "    print('The shape of Label is', lbl_input.shape, '\\n')\n",
    "\n",
    "\n",
    "    with open('data/enc_input.pickle', 'wb') as f:\n",
    "        pickle.dump(enc_input, f)\n",
    "    print('Encoder Input is created in \"data/enc_input.pickle\"\\n')\n",
    "\n",
    "    with open('data/dec_input.pickle', 'wb') as f:\n",
    "        pickle.dump(dec_input, f)\n",
    "    print('Decoder Input is created in \"data/dec_input.pickle\"\\n')\n",
    "\n",
    "    with open('data/lbl_input.pickle', 'wb') as f:\n",
    "        pickle.dump(lbl_input, f)\n",
    "    print('Label is created in \"data/lbl_input.pickle\"\\n')\n",
    "\n",
    "    with open('data/maxlen.pickle', 'wb') as f:\n",
    "        pickle.dump([maxlen_e, maxlen_d], f)\n",
    "    print('maxlens for encoder/decoder is created in \"data/maxlen.pickle\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Started making dictionaries\n",
      "\n",
      "The number of the dialog is 83075 \n",
      "\n",
      "Each data was 0 padded upto maxlen\n",
      "\n",
      "The shape of Encoder Input is (83033, 50, 1) \n",
      "\n",
      "The shape of Decoder Input is (83033, 50, 1) \n",
      "\n",
      "The shape of Label is (83033, 50, 1) \n",
      "\n",
      "Encoder Input is created in \"\"data/enc_input.pickle\"\"\n",
      "\n",
      "Decoder Input is created in \"\"data/dec_input.pickle\"\"\n",
      "\n",
      "Label is created in \"\"data/lbl_input.pickle\"\"\n",
      "\n",
      "maxlens for encoder/decoder is created in \"\"data/maxlen.pickle\"\"\n",
      "\n",
      "...completed making dictionaries\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Encoder Input、Decoder Input and Labels for training\n",
    "\n",
    "print('\\n...Started making dictionaries\\n')\n",
    "\n",
    "make_training_dataset('data/nucc_tokens.npy')\n",
    "\n",
    "print('...completed making dictionaries\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
