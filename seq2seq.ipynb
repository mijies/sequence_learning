{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from keras.initializers import glorot_uniform as keras_glorot_uniform\n",
    "from keras.initializers import orthogonal as keras_orthogonal\n",
    "from keras.initializers import uniform as keras_uniform\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Masking\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78881, 50, 1)\n",
      "(78881, 50)\n"
     ]
    }
   ],
   "source": [
    "# Prepeare data set\n",
    "\n",
    "with open('data/words.pickle', 'rb') as f:\n",
    "    words = pickle.load(f)\n",
    "    \n",
    "enc_input = np.load('data/enc_input.npy')\n",
    "dec_input = np.load('data/dec_input.npy')\n",
    "lbl_input = np.load('data/lbl_input.npy')\n",
    "maxlen_e, maxlen_d = np.load('data/maxlen.npy')\n",
    "\n",
    "\n",
    "# Split data for training and test\n",
    "sep_idx = int(enc_input.shape[0] * 0.95)\n",
    "enc_train, enc_test = np.vsplit(enc_input, [sep_idx])\n",
    "dec_train, dec_test = np.vsplit(dec_input, [sep_idx])\n",
    "lbl_train, lbl_test = np.vsplit(lbl_input, [sep_idx])\n",
    "\n",
    "print(enc_train.shape) # ?? 3rd dimension needed ??\n",
    "\n",
    "num = enc_train.shape[0]\n",
    "enc_train = enc_train.reshape(num, maxlen_e)\n",
    "dec_train = dec_train.reshape(num, maxlen_d)\n",
    "lbl_train = lbl_train.reshape(num, maxlen_d)\n",
    "\n",
    "print(enc_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dialog:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        maxlen_e,\n",
    "        maxlen_d,\n",
    "        num_input,\n",
    "        num_output,\n",
    "        dim_vec,\n",
    "        dim_hidden_vec\n",
    "    ):\n",
    "        self.maxlen_e = maxlen_e\n",
    "        self.maxlen_d = maxlen_d\n",
    "        self.num_input = num_input\n",
    "        self.num_output = num_output\n",
    "        self.dim_vec = dim_vec\n",
    "        self.dim_hidden_vec = dim_hidden_vec\n",
    "        \n",
    "        \n",
    "        \n",
    "    def create_model(self):        # Used in train() and prediction\n",
    "        \n",
    "        print('\\n...Start creating models.\\n')\n",
    "        \n",
    "        #\n",
    "        # Encoder model creation\n",
    "        #\n",
    "        \n",
    "        enc_input = Input(shape=(self.maxlen_e,), dtype='int32', name='encoder_input')\n",
    "        tf_tensor = Embedding(\n",
    "                              input_dim=self.num_input, \n",
    "                              output_dim=self.num_output, \n",
    "                              mask_zero=True, # ID 0 in Input data becomes a padding.(LSTM layer just passes the previous values)\n",
    "                              embeddings_initializer=keras_uniform(seed=12345)\n",
    "                             )(enc_input)\n",
    "        \n",
    "        # axis -1 counts dimension from the highest rank\n",
    "        tf_tensor = BatchNormalization(axis=-1)(tf_tensor)\n",
    "        \n",
    "        # When all IDs in a timestep is equal to mask_value, skipped in all downstream layers\n",
    "        # Maybe input data unlikely has a sequence whose all values are maske_value as corpus is parsed in such a way\n",
    "        tf_tensor = Masking(mask_value=0.0)(tf_tensor)\n",
    "        \n",
    "        enc_output, enc_hidden_state, enc_cell_state = LSTM(\n",
    "                                                            units=self.dim_hidden_vec,\n",
    "                                                            kernel_initializer=keras_glorot_uniform(seed=12345),\n",
    "                                                            # Coefficient to the orthogonal matrix is 1.0\n",
    "                                                            recurrent_initializer=keras_orthogonal(gain=1.0, seed=12345),\n",
    "                                                            dropout=0.5,\n",
    "                                                            recurrent_dropout=0.5,\n",
    "                                                            return_state=True,\n",
    "                                                           )(tf_tensor)\n",
    "        \n",
    "        enc_model  = Model(inputs=enc_input, outputs=[enc_output, enc_hidden_state, enc_cell_state])\n",
    "\n",
    "        print('Encoder model created.\\n')\n",
    "        \n",
    "        #\n",
    "        # Decoder training model creation\n",
    "        #\n",
    "        \n",
    "        dec_input = Input(shape=(self.maxlen_d,), dtype='int32', name='decoder_input')\n",
    "        tf_tensor = Embedding(\n",
    "                              input_dim=self.num_input,\n",
    "                              output_dim=self.dim_vec,\n",
    "                              mask_zero=True,\n",
    "                              embeddings_initializer=keras_uniform(seed=12345)\n",
    "                             )(dec_input)\n",
    "        \n",
    "        tf_tensor = BatchNormalization(axis=-1)(tf_tensor)\n",
    "        dec_LSTM_input = Masking(mask_value=0.0)(tf_tensor)\n",
    "        \n",
    "        # dec_LSTM is used later again\n",
    "        dec_LSTM = LSTM(\n",
    "                        units=self.dim_hidden_vec,\n",
    "                        kernel_initializer=keras_glorot_uniform(seed=12345),\n",
    "                        recurrent_initializer=keras_orthogonal(gain=1.0, seed=12345),\n",
    "                        dropout=0.5,\n",
    "                        recurrent_dropout=0.5,\n",
    "                        return_state=True,\n",
    "                        return_sequences=True,\n",
    "                       )\n",
    "        tf_tensor, _, _ = dec_LSTM(dec_LSTM_input, initial_state=[enc_hidden_state, enc_cell_state])\n",
    "        \n",
    "        # Densely connected NN after LSTM is used later again\n",
    "        dec_Dense = Dense(\n",
    "                          units=self.num_output,\n",
    "                          activation='softmax',\n",
    "                          kernel_initializer=keras_glorot_uniform(seed=12345)\n",
    "                         ) \n",
    "        dec_output = dec_Dense(tf_tensor)\n",
    "        \n",
    "        model = Model(inputs=[enc_input, dec_input], outputs=dec_output)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['categorical_accuracy'])\n",
    "        \n",
    "        print('Decoder training model created.\\n')\n",
    "        \n",
    "        #\n",
    "        # Decoder model creation\n",
    "        #\n",
    "        \n",
    "        dec_hidd_state_input = Input(shape=(self.dim_hidden_vec,), name='hidd_state_input')\n",
    "        dec_cell_state_input = Input(shape=(self.dim_hidden_vec,), name='cell_state_input')\n",
    "        \n",
    "        \n",
    "        tf_tensor, dec_hidden_state, dec_cell_state = dec_LSTM(\n",
    "                                                                dec_LSTM_input, \n",
    "                                                                initial_state=[dec_hidd_state_input, dec_cell_state_input]\n",
    "                                                                )\n",
    "        dec_response = dec_Dense(tf_tensor)\n",
    "        \n",
    "        dec_model = Model(\n",
    "                          inputs =[dec_input, dec_hidd_state_input, dec_cell_state_input],\n",
    "                          outputs=[dec_response, dec_hidden_state, dec_cell_state]\n",
    "                         )\n",
    "        \n",
    "        print('Decoder model created.\\n')\n",
    "        \n",
    "        return model, enc_model, dec_model\n",
    "    \n",
    "        \n",
    "    \n",
    "    def eval_perplexity(self,    # Used in on_batch() and test\n",
    "        model,\n",
    "        enc_validity,\n",
    "        dec_validity,\n",
    "        lbl_validity,\n",
    "        batch_size\n",
    "    ):\n",
    "    \n",
    "        return 1000\n",
    "    \n",
    "        \n",
    "#         for step in range(0, math.ceil(num_train / batch_size)):\n",
    "#             if step > 2:break\n",
    "            \n",
    "#             s = batch_size * step\n",
    "#             e = min(num_train, batch_size * (step + 1))\n",
    "            \n",
    "#             enc_batch = enc_train[s:e, :]\n",
    "#             dec_batch = dec_train[s:e, :]\n",
    "#             lbl_batch = lbl_train[s:e, :]\n",
    "\n",
    "            \n",
    "#             # turns IDs into binary values\n",
    "#             lbl_batch = np_utils.to_categorical(lbl_batch, self.num_output)\n",
    "            \n",
    "#             # Reshape the dimension into (1, N * maxlen) for cross entropy Loss dot product\n",
    "#             lbl_batch = lbl_batch.reshape(1,(e-s) * self.maxlen_d * self.num_output)\n",
    "            \n",
    "            \n",
    "# #             mask = np.zeros((e-s, self.maxlen_d, self.num_output), dtype=np.float32)\n",
    "# #             print(mask.shape)\n",
    "\n",
    "#             Y1 = model.predict_on_batch([enc_batch, dec_batch])\n",
    "#             Y2 = np.maximum(Y1, 1e-7)   # Replace the values less than 1e-7 with 1e-7\n",
    "#             Y2 = np.log(Y2)\n",
    "#             Y3 = Y2.reshape(1, (e-s) * self.maxlen_d * self.num_output)\n",
    "            \n",
    "#             Loss = np.dot(Y3, lbl_batch.T)\n",
    "        \n",
    "        \n",
    "\n",
    "    def on_batch(self,     # Used in train()\n",
    "         model, \n",
    "         epoch_idx, \n",
    "         enc_train,\n",
    "         dec_train,\n",
    "         lbl_train,\n",
    "         enc_validity,\n",
    "         dec_validity,\n",
    "         lbl_validity,\n",
    "         batch_size\n",
    "    ):\n",
    "                 \n",
    "        list_loss = []\n",
    "        list_acc  = []\n",
    "        num_train = enc_train.shape[0]\n",
    "\n",
    "        time_start = time.time()\n",
    "        \n",
    "        for step in range(0, math.ceil(num_train / batch_size)):\n",
    "            if step > 1:break\n",
    "            \n",
    "            s = batch_size * step\n",
    "            e = min(num_train, batch_size * (step + 1))\n",
    "            \n",
    "            enc_batch = enc_train[s:e, :]\n",
    "            dec_batch = dec_train[s:e, :]\n",
    "            lbl_batch = lbl_train[s:e, :]\n",
    "\n",
    "            \n",
    "            # turns IDs into binary values\n",
    "            lbl_batch = np_utils.to_categorical(lbl_batch, self.num_output)\n",
    "            \n",
    "            # train_on_batch returns [Loss scalar, Accuracy scalar]\n",
    "            result = model.train_on_batch([enc_batch, dec_batch], lbl_batch)\n",
    "            \n",
    "            list_loss.append(result[0])\n",
    "            list_acc.append(result[1])\n",
    "            \n",
    "            print(\n",
    "                  '\\n', e, '/', num_train, '\\t', str(int(time.time() - time_start)) + 'sec',\n",
    "                  '\\t', 'Loss :', \"{0:.4f}\".format(np.average(list_loss)),\n",
    "                  '\\t', 'Acc :',  \"{0:.4f}\".format(np.average(list_acc)),\n",
    "                 )\n",
    "            \n",
    "            del enc_batch, dec_batch, lbl_batch\n",
    "        \n",
    "        del list_loss, list_acc\n",
    "        \n",
    "        return self.eval_perplexity(\n",
    "                    model,\n",
    "                    enc_validity,\n",
    "                    dec_validity,\n",
    "                    lbl_validity,\n",
    "                    batch_size\n",
    "                )\n",
    "    \n",
    "        \n",
    "    \n",
    "    def train(self, enc_train, dec_train, lbl_train, epochs, batch_size, param_file):\n",
    "\n",
    "        model, _, _ = self.create_model()\n",
    "\n",
    "        if os.path.isfile(param_file):\n",
    "            model.load_weights(param_file)\n",
    "\n",
    "        for v in [enc_train, dec_train, lbl_train]:\n",
    "            np.random.seed(12345)\n",
    "            np.random.shuffle(v)\n",
    "            \n",
    "        \n",
    "        # Split data for training and validation\n",
    "        sep_idx = int(enc_train.shape[0] * 0.9)\n",
    "        enc_train, enc_validity = np.vsplit(enc_input, [sep_idx])\n",
    "        dec_train, dec_validity = np.vsplit(dec_input, [sep_idx])\n",
    "        lbl_train, lbl_validity = np.vsplit(lbl_input, [sep_idx])\n",
    "        \n",
    "        \n",
    "        # ?? Dimension changed from (N, maxlen) (N, maxlen, 1) ??\n",
    "        enc_train    = enc_train.reshape(enc_train.shape[0], self.maxlen_e)\n",
    "        enc_validity = enc_validity.reshape(enc_validity.shape[0], self.maxlen_e)\n",
    "        dec_train    = dec_train.reshape(dec_train.shape[0], self.maxlen_d)\n",
    "        dec_validity = dec_validity.reshape(dec_validity.shape[0], self.maxlen_d)\n",
    "        lbl_train    = lbl_train.reshape(lbl_train.shape[0], self.maxlen_d)\n",
    "        lbl_validity = lbl_validity.reshape(lbl_validity.shape[0], self.maxlen_d)\n",
    "        \n",
    "\n",
    "        early_stop_cap = 10000\n",
    "        \n",
    "        for epoch_idx in range(0, epochs):\n",
    "            \n",
    "            print('Epoch ', epoch_idx+1, '/', epochs) \n",
    "            validity_perplexity = self.on_batch(\n",
    "                                                model, \n",
    "                                                epoch_idx, \n",
    "                                                enc_train,\n",
    "                                                dec_train,\n",
    "                                                lbl_train,\n",
    "                                                enc_validity,\n",
    "                                                dec_validity,\n",
    "                                                lbl_validity,\n",
    "                                                batch_size\n",
    "                                               )\n",
    "            #EarlyStopping\n",
    "            if epoch_idx == 0 or validity_perplexity < early_stop_cap:\n",
    "                  early_stop_cap = validity_perplexity\n",
    "            else:\n",
    "                print('Early stopping\\n') \n",
    "                break\n",
    "                  \n",
    "        return model\n",
    "\n",
    "\n",
    "    \n",
    "    def sampling_detoknize(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Start creating models.\n",
      "\n",
      "Encoder model created.\n",
      "\n",
      "Decoder training model created.\n",
      "\n",
      "Decoder model created.\n",
      "\n",
      "Epoch  1 / 1\n",
      "\n",
      " 100 / 70992 \t 62sec \t Loss : 8.8768 \t Acc : 0.0000\n",
      "\n",
      " 200 / 70992 \t 82sec \t Loss : 8.6267 \t Acc : 0.3641\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 100\n",
    "param_file = 'param_seq2seq.hdf5'\n",
    "\n",
    "num_input = len(words)\n",
    "num_output = num_input\n",
    "dim_vec = 400\n",
    "dim_hidden_vec = int(dim_vec * 2)\n",
    "\n",
    "\n",
    "prediction = Dialog(maxlen_e, maxlen_d, num_input, num_output, dim_vec, dim_hidden_vec)\n",
    "\n",
    "model = prediction.train(enc_train, dec_train, lbl_train, epochs, batch_size, param_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
